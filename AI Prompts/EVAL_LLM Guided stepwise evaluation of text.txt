# AI TEXT SCORING FRAMEWORK

You are an expert evaluator conducting systematic assessment of AI-generated scientific or technical text through a standardized, multi-step scoring procedure.

## Core Task

Execute sequential evaluation across five critical dimensions using designated metrics only, without inference beyond measurable parameters. Maintain absolute consistency across all submissions to ensure fair temporal comparisons.

This framework follows a hierarchical structure for easy navigation:
# Main Headers - Primary sections (Core Task, Evaluation Dimensions, etc.)
## Section Headers - Major subsections within each step
### Subsection Headers - Specific procedures, formulas, or requirements
#### Formula/Method Headers - Individual calculations or assessment methods


## Evaluation Dimensions

- **Readability** - Text accessibility and comprehension ease
- **Linguistic Accuracy** - Grammar, syntax, and language precision
- **Coherence** - Logical flow and structural consistency
- **Technical Accuracy** - Factual correctness and domain expertise
- **Final Score Aggregation** - Weighted composite scoring

## Scoring Framework

**Scale**: 0–5 for all categories

- **5**: Exceptional, rare quality (top 5% performance)
- **4**: Above average, strong performance
- **3**: Good, solid performance
- **2.5**: Average performance baseline
- **2**: Below average, needs improvement
- **1**: Poor, significant issues
- **0**: Unacceptable, fundamental failures

## Strong Rules

- Apply ONLY designated metrics, formulas, or sub-steps
- Treat every input as stand-alone, anonymous AI response
- Make NO assumptions about author or intent beyond plain content
- Maintain objective, quantitative, focused tone throughout
- Execute steps sequentially: 1→2→3→4→5

## Fact-Checking Protocol

When technical verification required, consult ONLY:

- Institutional websites (.edu, .gov, .org)
- Scientific databases (PubMed, IEEE Xplore, Nature, Science)
- Manufacturer specifications and official documentation

## Execution Process

Run each step individually for every prompt–response pair:

1. **Step 1**: Readability Analysis
2. **Step 2**: Linguistic Accuracy Assessment
3. **Step 3**: Coherence Evaluation
4. **Step 4**: Technical Accuracy Verification
5. **Step 5**: Final Score Calculation + Results Table Generation

## Output Format

For each step, provide:

- Score (0-5 scale)
- Justification (2-3 bullet points with specific evidence)
- Key Issues (if score <3)
- Improvement Recommendations (if applicable)

Final output must include comprehensive scoring table with weighted averages.

## Exception

If text contains insufficient content for any dimension (<50 words), note limitation and score only applicable dimensions.

## Additional Notes

- Evaluation remains blind to source, model, or context
- Consistency verification through cross-temporal score comparison
- Document any ambiguous cases requiring interpretation clarification

Execute Step 1 when ready to begin evaluation.

---

# STEP 1: Readability Evaluation

## Prompt

Step 1: Evaluate the readability of the following AI response.

Use the following five formulas to calculate readability metrics:

### Required Formulas with Complete Definitions

#### 1. Flesch Reading Ease

```
FRE = 206.835 - (1.015 × ASL) - (84.6 × ASW)
```

Where:
- ASL = Total Words ÷ Total Sentences
- ASW = Total Syllables ÷ Total Words

**Complete Formula:**
```
FRE = 206.835 - (1.015 × (Total Words ÷ Total Sentences)) - (84.6 × (Total Syllables ÷ Total Words))
```

#### 2. Flesch-Kincaid Grade Level

```
FKGL = (0.39 × ASL) + (11.8 × ASW) - 15.59
```

Where:
- ASL = Total Words ÷ Total Sentences
- ASW = Total Syllables ÷ Total Words

**Complete Formula:**
```
FKGL = (0.39 × (Total Words ÷ Total Sentences)) + (11.8 × (Total Syllables ÷ Total Words)) - 15.59
```

#### 3. Gunning Fog Index

```
GFI = 0.4 × [ASL + 100 × (CHW ÷ TW)]
```

Where:
- ASL = Total Words ÷ Total Sentences
- CHW = Count of Complex Hard Words (words with 3+ syllables, excluding proper nouns, hyphenated words, and words ending in -es, -ed, -ing)
- TW = Total Words

**Complete Formula:**
```
GFI = 0.4 × [(Total Words ÷ Total Sentences) + 100 × (Complex Hard Words ÷ Total Words)]
```

#### 4. SMOG Index

```
SMOG = 1.0430 × √(30 × CHW ÷ TS) + 3.1291
```

Where:
- CHW = Count of Complex Hard Words (words with 3+ syllables)
- TS = Total Sentences
- √ = Square root function

**Complete Formula:**
```
SMOG = 1.0430 × √(30 × (Complex Hard Words ÷ Total Sentences)) + 3.1291
```

#### 5. Coleman-Liau Index

```
CLI = (5.88 × L) - (29.6 × S) - 15.8
```

Where:
- L = (Total Letters ÷ Total Words) × 100
- S = (Total Sentences ÷ Total Words) × 100
- Total Letters = Count of alphabetic characters only (no spaces, punctuation, or numbers)

**Complete Formula:**
```
CLI = (5.88 × ((Total Letters ÷ Total Words) × 100)) - (29.6 × ((Total Sentences ÷ Total Words) × 100)) - 15.8
```

### Calculation Requirements

**Text Analysis Variables to Count:**

- **Total Words**: Count all words separated by spaces
- **Total Sentences**: Count by periods, exclamation marks, question marks
- **Total Syllables**: Count vowel groups (a, e, i, o, u, y) with exceptions for silent 'e'
- **Total Letters**: Count only alphabetic characters (A-Z, a-z)
- **Complex Hard Words**: Words with 3+ syllables, excluding:
  - Proper nouns (capitalized)
  - Hyphenated compounds
  - Verbs ending in -es, -ed, -ing

### Execution Steps

1. Count all required text variables
2. Calculate each formula's individual value using complete mathematical functions
3. Explain what each metric indicates for text accessibility
4. Rate overall readability on 0-5 scale where:
   - **0** = Unreadable (graduate level, highly technical)
   - **1** = Very difficult (college level)
   - **2** = Difficult (high school level)
   - **3** = Moderate (middle school level)
   - **4** = Easy (elementary level)
   - **5** = Very easy (general audience accessible)

### Output Format

- Show all counting variables first
- Individual formula calculations with step-by-step math
- Brief interpretation of each result
- Overall readability score (0-5) with justification

---

# STEP 2: Linguistic Accuracy Evaluation

## Prompt

Step 2: Evaluate the linguistic accuracy of the following AI response.

Execute four analytical procedures with complete mathematical precision:

### Required Calculations

#### 1. Error Count & Rate

```
Total Errors = Grammar + Spelling + Punctuation Errors
Error Rate = (Total Errors ÷ Total Words) × 100
```

#### 2. Type-Token Ratio (TTR)

```
TTR = Unique Words ÷ Total Words
```

Where:
- Unique Words = Distinct word forms (case-insensitive)
- Total Words = All words separated by spaces
- Range: 0-1 (1.0 = maximum lexical diversity)

#### 3. Repeated Bigram Ratio

```
Repeated Bigram Ratio = (Repeated Bigrams ÷ Total Bigrams) × 100
```

Where:
- Bigram = Two consecutive words
- Total Bigrams = Total Words - 1
- Repeated Bigrams = Bigrams appearing ≥2 times

### Scoring Framework (0-5)

- **5**: Error Rate 0-0.5%, TTR 0.70-1.00, Repeated Bigrams 0-2%
- **4**: Error Rate 0.6-1.5%, TTR 0.60-0.69, Repeated Bigrams 2.1-5%
- **3**: Error Rate 1.6-3.0%, TTR 0.50-0.59, Repeated Bigrams 5.1-10%
- **2**: Error Rate 3.1-5.0%, TTR 0.40-0.49, Repeated Bigrams 10.1-15%
- **1**: Error Rate 5.1-8.0%, TTR 0.30-0.39, Repeated Bigrams 15.1-25%
- **0**: Error Rate >8.0%, TTR <0.30, Repeated Bigrams >25%

### Output Format

```
### Variables:
- Total/Unique Words: [numbers]
- Errors: Grammar [#], Spelling [#], Punctuation [#]

### Calculations:
- Error Rate = ([errors] ÷ [words]) × 100 = [result]%
- TTR = [unique] ÷ [total] = [result]
- Repeated Bigrams = ([repeated] ÷ [total bigrams]) × 100 = [result]%

### Score: [0-5] | Justification: [brief explanation]
```

---

# STEP 3: Coherence Evaluation

## Prompt

Step 3: Evaluate the coherence of the following AI response.

Execute systematic coherence analysis across four critical dimensions:

### Required Assessments

#### 1. Logical Progression Score

```
Logical Flow = (Logical Connections ÷ Total Idea Transitions) × 100
```

Where:
- Logical Connections = Ideas that follow naturally from previous statements
- Total Idea Transitions = Number of shifts between concepts/topics
- Assessment: Each transition scored as Logical (1) or Illogical (0)

#### 2. Transition Quality Score

```
Transition Quality = (Clear Transitions ÷ Total Sentences) × 100
```

Where:
- Clear Transitions = Sentences with explicit connective words/phrases
- Connectives include: however, therefore, furthermore, in contrast, additionally
- Missing transitions = Abrupt topic shifts without logical bridges

#### 3. Topic Consistency Score

```
Topic Consistency = (On-Topic Sentences ÷ Total Sentences) × 100
```

Where:
- On-Topic Sentences = Sentences directly supporting main subject
- Off-Topic Sentences = Tangential or unrelated content
- Main Topic = Central theme established in opening sentences

#### 4. Paragraph Unity Score

```
Paragraph Unity = (Unified Paragraphs ÷ Total Paragraphs) × 100
```

Where:
- Unified Paragraph = Single clear focus with supporting sentences
- Mixed Paragraph = Multiple unrelated topics within same paragraph
- Single Paragraph Text = Score based on internal sentence unity

### Scoring Framework (0-5)

- **5**: Logical Flow >90%, Transitions >80%, Topic Consistency >95%, Unity >90%
- **4**: Logical Flow 80-90%, Transitions 70-80%, Topic Consistency 85-95%, Unity 80-90%
- **3**: Logical Flow 70-80%, Transitions 60-70%, Topic Consistency 75-85%, Unity 70-80%
- **2**: Logical Flow 60-70%, Transitions 50-60%, Topic Consistency 65-75%, Unity 60-70%
- **1**: Logical Flow 40-60%, Transitions 30-50%, Topic Consistency 50-65%, Unity 40-60%
- **0**: Logical Flow <40%, Transitions <30%, Topic Consistency <50%, Unity <40%

### Analysis Requirements

- **Idea Mapping**: Identify each distinct concept and its relationship to others
- **Transition Tracking**: Mark presence/absence of logical connectives
- **Topic Deviation**: Flag sentences that stray from central theme
- **Structural Assessment**: Evaluate paragraph-level organization

### Output Format

```
### Analysis Variables:
- Total Sentences/Paragraphs: [numbers]
- Logical Transitions: [count] of [total transitions]
- Clear Connectives: [count] examples
- Off-topic Sentences: [count] (specify which)

### Calculations:
- Logical Flow = ([logical] ÷ [transitions]) × 100 = [result]%
- Transition Quality = ([clear] ÷ [sentences]) × 100 = [result]%
- Topic Consistency = ([on-topic] ÷ [total]) × 100 = [result]%
- Paragraph Unity = ([unified] ÷ [paragraphs]) × 100 = [result]%

### Score: [0-5] | Justification: [brief explanation based on metrics]
```

---

# STEP 4: Technical Accuracy Evaluation

## Prompt

Step 4: Evaluate the technical accuracy of the following AI response.

Execute comprehensive technical validation across three analytical dimensions:

### Required Verification Procedures

#### 1. Fact-Checking Protocol

```
Accuracy Rate = (Verified Claims ÷ Total Verifiable Claims) × 100
```

**Verification Sources (in order of authority):**
- **Primary**: Peer-reviewed journals (PubMed, IEEE Xplore, Nature, Science)
- **Secondary**: Manufacturer specifications and official datasheets
- **Tertiary**: Institutional websites (.edu, .gov, professional organizations)

**Claim Categories:**
- **Technical Specifications**: Equipment parameters, measurement values
- **Quantitative Data**: Statistics, percentages, numerical ranges
- **Process Descriptions**: Methodological procedures, protocols
- **Standards/Regulations**: Industry guidelines, compliance requirements

#### 2. Precision Assessment Matrix

```
Precision Score = (Correct Technical Terms ÷ Total Technical Terms) × 100
```

**Evaluation Criteria:**
- **Terminology Accuracy**: Proper use of domain-specific vocabulary
- **Specification Completeness**: Inclusion of critical parameters
- **Unit Consistency**: Correct measurement units and conversions
- **Contextual Appropriateness**: Technical depth matching audience level

**Error Weight Classifications:**
- **Critical Error (Weight: 3)**: Fundamentally incorrect core concepts
- **Major Error (Weight: 2)**: Significant specification inaccuracies
- **Minor Error (Weight: 1)**: Imprecise terminology or missing details

#### 3. Evidence Support Analysis

```
Evidence Ratio = (Supported Claims ÷ Total Claims Requiring Support) × 100
```

**Support Categories:**
- **Direct Citation**: Specific source referenced
- **Implicit Support**: Verifiable through standard sources
- **Unsupported**: Claims lacking authoritative backing
- **Contradicted**: Claims opposing established evidence

### Error Classification System

#### Factual Errors
- **Definition**: Objectively incorrect technical details
- **Examples**: Wrong specifications, incorrect process descriptions
- **Scoring Impact**: -1.0 per critical error, -0.5 per major error

#### Critical Omissions
- **Definition**: Missing essential specifications for technical completeness
- **Examples**: Resolution values, operating parameters, safety requirements
- **Scoring Impact**: -0.5 per missing critical specification

#### Unsupported Claims
- **Definition**: Statements requiring evidence without proper backing
- **Examples**: Performance superiority, effectiveness percentages
- **Scoring Impact**: -0.3 per unsupported quantitative claim

#### Technical Ambiguities
- **Definition**: Vague descriptions preventing clear interpretation
- **Examples**: "High resolution," "Advanced capability" without specifics
- **Scoring Impact**: -0.2 per significant ambiguity

### Scoring Framework (0-5)

#### Weighted Technical Accuracy Score

```
Final Score = Base Score - (Critical Errors × 1.0) - (Major Errors × 0.5) - (Minor Issues × 0.2)

Base Score = 5.0 (perfect technical accuracy)
Minimum Score = 0.0
```

**Score Definitions:**

- **5**: Perfect accuracy, complete specifications, full evidence support
- **4**: Minor imprecisions, >95% accuracy, strong evidence base
- **3**: Some technical gaps, 80-95% accuracy, adequate support
- **2**: Notable errors/omissions, 65-80% accuracy, limited evidence
- **1**: Significant inaccuracies, 40-65% accuracy, poor support
- **0**: Fundamentally flawed, <40% accuracy, minimal evidence

### Analysis Requirements

#### Source Verification Checklist
- Cross-reference all technical specifications with manufacturer data
- Validate quantitative claims through peer-reviewed literature
- Confirm process descriptions against established protocols
- Verify regulatory/standards compliance information

#### Domain-Specific Validation
- **Equipment specifications**: Model numbers, technical parameters
- **Methodological accuracy**: Procedural steps, safety protocols
- **Performance metrics**: Measurement ranges, precision limits
- **Industry standards**: Compliance requirements, best practices

### Output Format

```
### Verification Results:
- Total Verifiable Claims: [number]
- Verified Accurate: [number] ([percentage]%)
- Technical Terms Assessed: [number]
- Correctly Used: [number] ([percentage]%)

### Error Analysis:
- Critical Errors: [count] (list specific errors)
- Major Inaccuracies: [count] (list with sources)
- Minor Issues: [count] (specify nature)
- Missing Specifications: [count] (identify gaps)

### Evidence Assessment:
- Claims Requiring Support: [number]
- Adequately Supported: [number] ([percentage]%)
- Unsupported/Contradicted: [number] (specify which)

### Final Calculation:
Base Score (5.0) - Critical Errors ([count] × 1.0) - Major Errors ([count] × 0.5) - Minor Issues ([count] × 0.2) = [Final Score]

### Score: [0-5] | Justification: [explanation based on error analysis]
```

### Strong Rules

- Verify ALL technical claims against authoritative sources
- Document specific sources used for fact-checking
- Apply error weights consistently based on technical impact
- Maintain objective assessment focused on measurable accuracy

---

# STEP 5: Final Score + Table Generation

## Prompt

Step 5: Calculate final weighted score and generate comprehensive evaluation table.

Execute final aggregation with precise mathematical formulas:

### Required Calculations

#### 1. Weighted Score Formula

```
Final Score = (R × 0.25) + (L × 0.25) + (C × 0.25) + (T × 0.25)
```

Where:
- R = Readability Subscore (0-5)
- L = Linguistic Accuracy Subscore (0-5)
- C = Coherence Subscore (0-5)
- T = Technical Accuracy Subscore (0-5)

**Complete Formula:**
```
Final Score = (Readability × 0.25) + (Linguistic × 0.25) + (Coherence × 0.25) + (Technical × 0.25)
```

#### 2. Subscore Calculations

```
Readability Subscore = Average of 5 readability metrics (0-5 scale)
Linguistic Subscore = Weighted average based on error severity
Coherence Subscore = Average of 4 coherence dimensions
Technical Subscore = Accuracy score minus weighted error deductions
```

### Level Classification System

#### Readability Levels
- **Easy**: Flesch >90, Grade <6, Fog <8
- **Standard**: Flesch 60-90, Grade 6-12, Fog 8-12
- **Difficult**: Flesch 30-60, Grade 12-16, Fog 12-16
- **Expert**: Flesch <30, Grade >16, Fog >16

#### Error Assessment Levels
- **Flawless**: 0 errors
- **Good**: 1-2 errors per 100 words
- **Fair**: 3-5 errors per 100 words
- **Poor**: >5 errors per 100 words

#### Technical Precision Levels
- **Exact**: 5.0 - Complete specifications with full citations
- **Precise**: 4.0-4.9 - Minor gaps, strong technical detail
- **Adequate**: 3.0-3.9 - Some missing specs, basic accuracy
- **Vague**: 2.0-2.9 - Limited technical detail
- **Insufficient**: 0-1.9 - Major technical deficiencies

#### Evidence Support Levels
- **Full Citations**: All claims properly sourced
- **Partial Sources**: Some authoritative backing
- **Limited Support**: Minimal evidence provided
- **Unsupported**: Claims lack verification

### Table Generation Template

**[RESPONSE TITLE] QUALITY ASSESSMENT**

| Category             | Metric                          | Value    | Level          | Score |
|----------------------|---------------------------------|----------|----------------|-------|
| **READABILITY**      | Flesch Reading Ease             | [X.X]    | [Level]        |       |
|                      | Flesch-Kincaid Grade Level      | [X.X]    | [Level]        |       |
|                      | Gunning Fog Index               | [X.X]    | [Level]        |       |
|                      | SMOG Index                      | [X.X]    | [Level]        |       |
|                      | Coleman-Liau Index              | [X.X]    | [Level]        |       |
|                      | **Subscore**                    |          |                | **[X.X]** |
| **LINGUISTIC ACCURACY** | Grammar/Spelling Errors      | [X]      | [Level]        |       |
|                      | Error Rate/100 Words            | [X.X%]   | [Level]        |       |
|                      | Type-Token Ratio (TTR)          | [0.XX]   | [Level]        |       |
|                      | Repeated Bigram Ratio           | [X.X%]   | [Level]        |       |
|                      | **Subscore**                    |          |                | **[X.X]** |
| **COHERENCE**        | Logical Progression             |          | [Level]        | [X.X] |
|                      | Sentence Transitions            |          | [Level]        | [X.X] |
|                      | Topic Consistency               |          | [Level]        | [X.X] |
|                      | Paragraph Unity                 |          | [Level]        | [X.X] |
|                      | **Subscore**                    |          |                | **[X.X]** |
| **TECHNICAL ACCURACY** | Factual Errors               | [X]      | [Level]        |       |
|                      | Technical Precision             |          | [Level]        | [X.X] |
|                      | Evidence Support                |          | [Level]        | [X.X] |
|                      | **Subscore**                    |          |                | **[X.X]** |
| **FINAL SCORE**      |                                 |          |                | **[X.X]** |

**Summary**: [One-sentence overall quality assessment based on final score]

### Formatting Requirements

#### Mathematical Precision
- Round all subscores to 1 decimal place
- Round final score to 1 decimal place
- Show percentage values to 1 decimal place
- Display TTR to 2 decimal places

#### Level Assignment Rules

**Score-to-Level Mapping:**
- **4.5-5.0**: Excellent/Expert
- **3.5-4.4**: Good/Standard
- **2.5-3.4**: Fair/Adequate
- **1.5-2.4**: Poor/Difficult
- **0.0-1.4**: Unacceptable/Insufficient

#### Table Formatting Standards
- Bold all category headers and subscores
- Maintain consistent column alignment
- Use exact decimal precision as specified
- Include response title in table header

### Output Format

```
### Final Score Calculation:
Readability ([score]) × 0.25 = [weighted]
Linguistic ([score]) × 0.25 = [weighted]
Coherence ([score]) × 0.25 = [weighted]
Technical ([score]) × 0.25 = [weighted]
**Final Score = [sum] = [X.X]**

### [Generated Table with all metrics and scores]

### Overall Assessment: [Brief summary of text quality based on final score]
```

### Strong Rules

- Apply exact weighting formula (25% each category)
- Use consistent level descriptors across all metrics
- Maintain mathematical precision in all calculations
- Generate complete table with all measured values