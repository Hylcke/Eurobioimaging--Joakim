This study employed a systematic five-step evaluation framework to assess AI-generated scientific and technical text quality across four critical dimensions: readability, linguistic accuracy, coherence, and technical accuracy. The methodology was designed to ensure objective, quantitative assessment through standardized metrics and consistent scoring procedures.



**Text Analysis Protocol**: Each evaluation followed a sequential five-step process. Step 1 assessed readability using five established formulas: Flesch Reading Ease, Flesch-Kincaid Grade Level, Gunning Fog Index, SMOG Index, and Coleman-Liau Index. Text variables including total words, sentences, syllables, letters, and complex words (3+ syllables) were systematically counted. Readability scores were mapped to a 0-5 scale based on grade-level accessibility.



**Linguistic Accuracy Assessment**: Step 2 quantified linguistic precision through error rate calculation (grammar, spelling, punctuation errors per 100 words), Type-Token Ratio for lexical diversity, and Repeated Bigram Ratio for content repetitiveness. Each metric was weighted according to predetermined thresholds and converted to the standardized 0-5 scale.



**Coherence Evaluation**: Step 3 analyzed textual organization across four dimensions: logical progression between ideas, transition quality using explicit connectives, topic consistency relative to established themes, and paragraph unity. Percentage-based calculations measured the proportion of logical connections, clear transitions, on-topic sentences, and unified paragraphs.



**Technical Verification**: Step 4 employed fact-checking protocols using peer-reviewed sources (PubMed, IEEE Xplore, Nature, Science), manufacturer specifications, and institutional databases. Technical accuracy was calculated using weighted error deduction from a base score of 5.0, with critical errors (-1.0), major errors (-0.5), and minor issues (-0.2) systematically categorized.



**Score Aggregation**: Step 5 computed final weighted scores using equal weighting (25% per dimension): Final Score = (Readability × 0.25) + (Linguistic × 0.25) + (Coherence × 0.25) + (Technical × 0.25). All calculations maintained one decimal precision, with comprehensive results presented in standardized tabular format.



**Quality Assurance**: Evaluators applied only designated metrics without subjective interpretation, treating all submissions as anonymous to ensure temporal consistency and eliminate bias. The framework's mathematical precision and systematic approach enabled reliable cross-temporal comparisons of AI text quality.

